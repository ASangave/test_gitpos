apiVersion: v1
data:
  prometheus-alerts.yaml: |
    groups:
    - name: k8s_nodes
      rules:
        # Disabled by Josh Brown on 11/20/2018
        # Load is processes in the run queue, we have a densely packed set of process that run on each node.
        # Each node has a certain number of CPUs that provide the scheduler a maximum number of CPU resources that
        # the node can support.  We set resource requests to values that generally reflect an averaged estimation
        # of what the service requires to run.  Some services, outside of Kube's control, also require CPU and thus
        # can cause the run queue to grow as well (kubelet for example).  So... The main point here is that..
        # Processes in the run queue are not bad.  Load averages are really only bad if processes are blocked or system
        # cpu time is being consumed during things like cache (page or slab) eviction.  Resource requests + limits should
        # prevent a node from truly being overscheduled.  We can monitor blocked processes (node_procs_blocked) and
        # processes doing a lot of system calls with (container_cpu_system_seconds_total).
        #- alert: k8s_node_high_load
        #  expr: (sum(node_load1) by (instance)) > 20
        #  for: 20m
        #  labels:
        #    severity: "critical"
        #  annotations:
        #    summary: "Instance {{ $labels.instance }} has load of above 20"
        #    description: "{{ $labels.instance }} of job {{ $labels.job }} has had load of above 20 for more than 15 minutes."
        - alert: k8s_node_exporter_down
          expr: up{job="kubernetes-node-node-exporter"} == 0
          for: 10m
          labels:
            service: "k8s"
            severity: "warning"
          annotations:
            identifier: '{{ $labels.node }}'
            summary: "node-exporter cannot be scraped"
            description: "Prometheus could not scrape a node-exporter on {{ $labels.instance }}"
        - alert: k8s_node_out_of_disk
          expr: kube_node_status_condition{condition="OutOfDisk", status="true"} == 1
          labels:
            service: "k8s"
            severity: "critical"
          annotations:
            identifier: '{{ $labels.node }}'
            summary: "Node ran out of disk space."
            description: "{{ $labels.node }} has run out of disk space."
        - alert: k8s_node_kernel_deadlock
          expr: kube_node_status_condition{condition="KernelDeadlock", status="true"} == 1
          labels:
            service: "k8s"
            severity: "critical"
          annotations:
            identifier: '{{ $labels.node }}'
            summary: "Node has a kernel deadlock."
            description: "{{ $labels.node }} has experienced a kernel deadlock."
        - alert: k8s_node_memory_pressure
          expr: kube_node_status_condition{condition="MemoryPressure", status="true"} == 1
          labels:
            service: "k8s"
            severity: "warning"
          annotations:
            identifier: '{{ $labels.node }}'
            summary: "Node is under memory pressure."
            annotations: "{{ $labels.node }} is under memory pressure."
        - alert: k8s_node_disk_pressure
          expr: kube_node_status_condition{condition="DiskPressure", status="true"} == 1
          for: 10m
          labels:
            service: "k8s"
            severity: "warning"
          annotations:
            identifier: '{{ $labels.node }}'
            summary: "Node is under disk pressure."
            annotations: "{{ $labels.node }} is under disk pressure."
        - alert: k8s_node_not_ready
          expr: kube_node_status_condition{condition="Ready", status="true"} == 0
          for: 1h
          labels:
            service: "k8s"
            severity: "warning"
          annotations:
            identifier: '{{ $labels.node }}'
            summary: "Node status is NotReady"
            annotations: "The Kubelet on {{ $labels.node }} has not checked in with the API, or has set itself to NotReady, for more than an hour"
        - alert: k8s_many_nodes_not_ready
          expr: |
            count by (cluster) (kube_node_status_condition{condition="Ready", status="true"} == 0) > 1 AND
            (count by (cluster) (kube_node_status_condition{condition="Ready", status="true"} == 0) /
            count by (cluster) (kube_node_status_condition{condition="Ready", status="true"}) ) > 0.2
          for: 5m
          labels:
            service: "k8s"
            severity: "critical"
          annotations:
            summary: "Many Kubernetes nodes are not ready"
            description: "{{ $value }} nodes (more than 10% of cluster) are in the NotReady state."
        - alert: k8s_kubelet_pleg_problem
          expr: |
            kubelet_pleg_relist_latency_microseconds > 1000000
          for: 15m
          labels:
            service: "k8s"
            severity: "warning"
          annotations:
            summary: "Node has PLEG or eth0 problem"
            description: "Node has PLEG relist latency higher than {{ $value }} for more than 15 minutes. Check PLEG and eth0 errors."


    - name: k8s_apiservers
      rules:
        - alert: k8s_apiserver_down
          expr: up{job="kubernetes-apiservers"} == 0
          for: 1m
          labels:
            service: "k8s"
            severity: "warning"
          annotations:
            summary: "Kubernetes API server unreachable."
            description: "A Kubernetes API server could not be scraped."
        - alert: k8s_apiservers_down
          expr: absent({job="kubernetes-apiservers"}) or (count by(instance) (up{job="kubernetes-apiservers"} == 1) < count by(instance) (up{job="kubernetes-apiservers"}))
          for: 5m
          labels:
            service: "k8s"
            severity: "critical"
          annotations:
            summary: "Kubernetes API server unreachable."
            description: "Prometheus failed to scrape multiple API servers, or all API servers have disappeared from service discovery."

    - name: k8s_kubelets
      rules:
        - alert: k8s_kubelet_down
          expr: up{job="kubernetes-nodes"} == 0
          for: 1h
          labels:
            service: "k8s"
            severity: "warning"
          annotations:
            summary: "Kubelet cannot be scraped"
            description: "Prometheus could not scrape a kubelet for more than one hour"
        - alert: k8s_kubelets_down
          expr: absent(up{job="kubernetes-nodes"}) or count by (instance) (up{job="kubernetes-nodes"} == 0) / count by (instance) (up{job="kubernetes-nodes"}) > 0.1
          for: 1h
          labels:
            service: "k8s"
            severity: "critical"
          annotations:
            summary: "Many Kubelets cannot be scraped"
            description: "Prometheus failed to scrape more than 10% of kubelets, or all Kubelets have disappeared from service discovery."

    - name: k8s_pod_status
      rules:
        - alert: k8s_pods_in_waiting_state
          expr: sum(kube_pod_container_status_waiting == 1) by (namespace, pod)
          for: 15m
          labels:
            service: "k8s"
            severity: "warning"
          annotations:
            identifier: '{{ $labels.pod }}'
            summary: "Pods in waiting state"
            description: "â€¢ Pod (`{{ $labels.pod }}`) in NS (`{{ $labels.namespace }}`)"
            runbook: "https://sightmachine.atlassian.net/wiki/spaces/ops/pages/527433772/Infrastructure%2BEngineering%2BOn-Call"

    - name: k8s_file_descriptors
      rules:
        - alert: k8s_fd_exhaustion_close
          expr: predict_linear(instance:fd_utilization[1h], 3600 * 4) > 1
          for: 10m
          labels:
            service: "k8s"
            severity: "warning"
          annotations:
            summary: "File descriptors soon exhausted."
            description: "{{ $labels.job }} instance {{ $labels.instance }} will exhaust in file descriptors soon."
        - alert: k8s_fd_exhaustion_close
          expr: predict_linear(instance:fd_utilization[10m], 3600) > 1
          for: 10m
          labels:
            service: "k8s"
            severity: "critical"
          annotations:
            summary: "file descriptors soon exhausted."
            description: "{{ $labels.job }} instance {{ $labels.instance }} will exhaust in file descriptors within the next hour."
        - alert: k8s_too_many_open_files
          expr: 100*process_open_fds{job=~"kubernetes-apiservers|kubernetes-nodes"} / process_max_fds > 50
          for: 10m
          labels:
            service: "k8s"
            severity: "warning"
          annotations:
            summary: "{{ $labels.job }} has too many open file descriptors"
            description: "{{ $labels.node }} is using {{ $value }}% of the available file/socket descriptors."
        - alert: k8s_too_many_open_files
          expr: 100*process_open_fds{job=~"kubernetes-apiservers|kubernetes-nodes"} / process_max_fds > 80
          for: 10m
          labels:
            service: "k8s"
            severity: "critical"
          annotations:
            summary: "{{ $labels.job }} has too many open file descriptors"
            description: "{{ $labels.node }} is using {{ $value }}% of the available file/socket descriptors."

    - name: ma
      rules:
        - alert: ma_frontend_not_200_ok
          expr: probe_http_status_code{job="kubernetes-ingresses",kubernetes_namespace=~"mase-.+",kubernetes_name="nginx"} != 200
          for: 5m
          labels:
            service: "frontend"
            severity: "warning"
          annotations:
            summary: "web frontend not reporting 200 OK on {{ $labels.kubernetes_namespace }}"
            description: "web frontend not reporting 200 OK {{ $labels.kubernetes_namespace }}"
        - alert: ma_celery_rabbitmq_queue_message_response_seconds_late
          expr: |
            (ma_rabbitmq_queue_message_response_seconds_late{kubernetes_namespace=~"mase-(demo|.*prometheus-test)"} != 0 or
            ma_rabbitmq_queue_message_response_seconds_late{voltron_environment_function="production"} != 0) and
            ma_rabbitmq_queue_message_response_seconds_late{queue=~".*celery.*"} and
            ma_rabbitmq_queue_message_response_seconds_late{queue!~".*ma\\.etl\\..*"}
          for: 1m
          labels:
            service: "ma"
          annotations:
            identifier: '{{ $labels.queue }}'
            summary: "{{$labels.kubernetes_namespace}} {{$labels.queue}}: Message in queue is late."
            description: "{{$labels.kubernetes_namespace}} {{$labels.queue}}: Message in queue is late."
    - name: etl
      rules:
        ### For backend ma metrics, consider the polling interval when adding in these alerts.
        - alert: ma_etl_monitor_error_machines_state
          expr: |
            ma_etl_monitor_error_machines_state{kubernetes_namespace=~"mase-(demo|.*prometheus-test)"} == 1 or
            ma_etl_monitor_error_machines_state{voltron_environment_function="production"} == 1
          for: 1m
          labels:
            service: "etl"
          annotations:
            summary: "{{$labels.kubernetes_namespace}} {{$labels.machine}}: In error state."
            description: "{{$labels.kubernetes_namespace}} {{$labels.machine}}: In error state."
        - alert: ma_etl_inactive
          expr: |
            (
              delta(ma_etl_status_updatetime{kubernetes_namespace=~"mase-(demo|.*prometheus-test)"}[5m]) <= 0 or
              delta(ma_etl_status_updatetime{voltron_environment_function="production"}[5m]) <= 0
            ) and
            on(kubernetes_namespace, voltron_environment_function, machine)
            (
              ma_etl_monitor_error_machines_state{kubernetes_namespace=~"mase-(demo|.*prometheus-test)"} != 1 or
              ma_etl_monitor_error_machines_state{voltron_environment_function="production"} != 1
            )
          for: 2h
          labels:
            service: "etl"
          annotations:
            identifier: '{{ $labels.machine }}'
            summary: "{{$labels.kubernetes_namespace}} {{$labels.machine}}: etl processing has not progressed."
            description: "{{$labels.kubernetes_namespace}} {{$labels.machine}}: etl processing has not progressed."
        - alert: ma_etl_rabbitmq_queue_message_response_seconds_late
          expr: |
            (ma_rabbitmq_queue_message_response_seconds_late{kubernetes_namespace=~"mase-(demo|.*prometheus-test)"} != 0 or
              ma_rabbitmq_queue_message_response_seconds_late{voltron_environment_function="production"} != 0) and
            (ma_rabbitmq_queue_message_response_seconds_late{queue!~".*celery.*"} or
              ma_rabbitmq_queue_message_response_seconds_late{queue=~".*ma\\.etl\\..*"}
            )
          for: 1m
          labels:
            service: "etl"
          annotations:
            identifier: '{{ $labels.queue }}'
            summary: "{{$labels.kubernetes_namespace}} {{$labels.queue}}: Message in queue is late."
            description: "{{$labels.kubernetes_namespace}} {{$labels.queue}}: Message in queue is late."
        - alert: ma_etl_monitor_last_sslog_error_seconds_master_demo_environments
          expr: max(ma_etl_monitor_last_sslog_error_seconds{kubernetes_namespace=~"mase-demo|mase-ema-demo|mase-emv-demo"}) by (kubernetes_namespace) > 0
          for: 1m
          labels:
            service: "demo"
          annotations:
            identifier: '{{ $labels.kubernetes_namespace }}'
            summary: "We have not seen new sslogs for {{$labels.kubernetes_namespace}}"
            description: "We have not seen new sslogs for {{$labels.kubernetes_namespace}}, please check linedatasimulator and related configuration"
    - name: factorytx
      rules:
        - alert: ftx_marker_status
          expr: ftx_marker_status{ftx_level="error"} == 1
          for: 1m
          labels:
            service: "factorytx"
          annotations:
            summary: "{{$labels.ftx_level}}: {{$labels.ftx_component}} ({{$labels.ftx_component_type}}) -- {{$labels.kubernetes_pod_name -}} {{if $labels.voltron_environment_function}} ({{$labels.voltron_environment_function}}){{end}}"
            description: "*{{$labels.ftx_category}}:* {{$labels.ftx_message}}"

    - name: fluentd
      rules:
        - alert: fluentd_buffers_filling_up
          expr: fluentd_output_status_buffer_queue_length{app="fluentd-elasticsearch-fluentd-elasticsearch"} > 10000
          for: 15m
          labels:
            service: "fluentd"
            severity: "warning"
          annotations:
            identifier: '{{ $labels.kubernetes_namespace }}'
            summary: "fluentd buffers filling up {{ $labels.kubernetes_namespace }}"
            description: "The buffers are filling which means logs probably are not shipping or behind {{ $labels.__meta_kubernetes_pod_name  }}"
kind: ConfigMap
metadata:
  name: prometheus-alerts
  namespace: prometheus
